
<span class="anchor" id="about-me"></span>

## ğŸ‘‹ About Me

I am currently a first-year Ph.D. student (Fall 2025 intake) in Artificial Intelligence at the [School of Computing and Data Science (CDS), The University of Hong Kong (HKU)](https://www.cds.hku.hk/), advised by [Prof. Difan Zou](https://difanzou.github.io/). Previously, I earned my Bachelorâ€™s degree in 2023 from the University of Electronic Science and Technology of China (UESTC) and obtained my Masterâ€™s degree in 2025 from the Faculty of Science, The University of Hong Kong.

My research focuses on **AI Interpretability, Trustworthy and Safety**, aiming to better understand Large Language Models (LLMs) and to design more advanced and safer AI. In particular, I am actively exploring [**Circuit Analysis**](https://arxiv.org/abs/2502.11812) and [**Sparse Autoencoders (SAEs)**](https://arxiv.org/abs/2510.03659), with the long-term goal of (1) understanding how LLMs work internally, (2) improving their performance, and (3) building models that are safer and more controllable.

If you would like to get in touch or share a passion for LLM interpretability and safety, or if you are looking for potential collaboration in this direction, feel free to reach out via email: **sunny615@connect.hku.hk**!


---

<span class="anchor" id="news"></span>

## ğŸ—ï¸ News

- ğŸ“ *[2025.08]* Two Paper [**Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models**](https://arxiv.org/abs/2505.15634) and [**Model Unlearning via Sparse Autoencoder Subspace Guided Projections**](https://arxiv.org/abs/2505.24428) have been accepted at **EMNLP 2025**
- ğŸ“ *[2025.05]* Paper [**Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis**](https://arxiv.org/abs/2502.11812) accepted at **ICML 2025**

---

<span class="anchor" id="publications"></span>

## ğŸ“„ Publications

### Part 1: Conference Publications

- **Xu Wang**, Z Li, B Wang, Y Hu, D Zou. *Model Unlearning via Sparse Autoencoder Subspace Guided Projections*  
  _EMNLP 2025 (accepted)_

- Li, Z, **Xu Wang**, Y Yang, Z Yao, H Xiong, M Du. *Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models*  
  _EMNLP 2025 (accepted)_

- **Xu Wang**, et al. *Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis*  
  _ICML 2025 (accepted)_

- Cheung, L, **Xu Wang**, J Zhang, RKM Poon, ASM Lau. *Applications of Generative AI: A Case Study of AI Doctor*  
  _Southeast Decision Sciences Institute (SEDSI) Conference, 29 Jan 2025 â€“ 31 Jan 2025 (accepted)_

---

### Part 2: Preprints on arXiv

- **Xu Wang**, Y Hu, B Wang, D Zou. *Does Higher Interpretability Imply Better Utility? A Pairwise Analysis on Sparse Autoencoders*  
  [_arXiv:2510.03659_](https://arxiv.org/abs/2510.03659)

---

<span class="anchor" id="experience"></span>

## ğŸ”¬ Experience

- **HKU Research Assistant, Department of Statistics and Actuarial Science** (05/2024 â€“ 08/2024)  
  _Research Direction: LLM applications in healthcare._

- **CUHK (ShenZhen) Research Assistant, School of Data Science** (09/2024 â€“ 08/2025)  
  _Research Direction: LLM mechanistic interpretability and AI safety._


---

<span class="anchor" id="services"></span>

## ğŸ§© Services (Conference Reviewers)

- Reviewer for **ICLR**
- Reviewer for **EMNLP**

---

<span class="anchor" id="future-plan"></span>

## ğŸ§­ Future Plan

- ğŸ” Continue exploring **LLM interpretability**, following [Anthropic Interpretability team](https://www.anthropic.com/research#interpretability)
- ğŸ§  Leverage **SAE**, **Circuit**, and related methods to uncover the internal mechanisms of LLMs, delivering improved foundational SAE and features to the community
- ğŸ›¡ï¸ Continue exploring **AI safety**, focusing on data security and training robustness in LLMs
- ğŸŒ Combine mechanistic interpretability with inference and reasoning: identify ways to integrate inference scaling and reinforcement learning (RL) theory into mechanistic interpretability research



